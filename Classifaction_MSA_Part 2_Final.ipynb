{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Classifaction MSA Part 2\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Checking_status: This feature is a categorical variable that describes the status of the current checking account of the individual. \n\nDuration: This feature is a numerical variable describing the duration in months of the credit.\n\nCredit_history: This categorical feature describes the individual's credit history; this feature is crucial as it shows banks if they have had loans before and if they have paid them off.\n\nPurpose: This is a categorical feature, and it is given to banks showing them the purpose for which the credit is taken, for example, if they need to buy a car or a house.\n\nCredit_amount: This is a numerical feature representing the credit amount.\n\nSavings_status: This is a categorical feature that indicates the savings status of the person applying for the loan. It can be concluded that the higher the savings, the better.\n\nEmployment: This is a categorical feature that describes the employment duration, for example, how many years they have worked or even if they are unemployed.\n\nInstallment_commitment: This is the instalment rate in percentage of disposable income. It is a numerical variable.\n\nPersonal_status: This categorical feature describes the personal status, for example, what gender they are and whether they are married.\n\nOther_parties: This categorical variable indicates if other debtors/guarantors are involved in the life of the person applying for a loan, it is possible that \"None\" is involved.\n\nResidence_since: This numerical feature is the present residence in years.\n\nProperty_magnitude: This categorical variable describes the property people looking for loans have, such as 'real estate', 'life insurance', and 'unknown/no property'.\n\nAge: This is a numerical variable indicating the age of the individual applying for a loan.\n\nOther_payment_plans: This categorical variable indicates other instalment plans such as 'bank', 'stores', and 'none'.\n\nHousing: This categorical variable which indicates the housing situation, for example, if they are \"renting\" or \"owning\" their home (banks can use this to show if the individual is living in a \"stable\" environment\").\n\nExisting_credits: This numerical feature shows the number of existing credits at this bank.\n\nJob: This categorical variable describes the job situation ranging from unemployed/unskilled' to 'highly skilled'.\n\nNum_dependents: This numerical feature shows the number of people liable for maintenance.\n\nOwn_telephone: This categorical variable tells the bank if the individual has a phone.\n\nForeign_worker: This categorical variable indicates if the individual is a foreign worker.\n\nClass: This is the target variable that the model will try and predict, given the other features in the data set. It is also a categorical variable representing whether the credit risk is 'good' or 'bad'.\n\nThese are the variables that are all the ones provided in the dataset. It includes both numerical and categorical data. These features can be used to train a machine learning model to predict the target variable once the categorical is turned into numerical. The class will be the targeted variable as it is the most appropriate variable to let banks know if they should give out the loan to the individual.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Importing libaries and removing outliers\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\n# Load data\ndf_credit_risk = pd.read_csv('credit_risk.csv')\n\ndef remove_outliers(df):\n    numeric_cols = df.select_dtypes(include=[np.number])\n    mean = numeric_cols.mean()\n    std = numeric_cols.std()\n    is_outlier = (np.abs(numeric_cols - mean) > 3 * std).any(axis=1)\n    return df[~is_outlier]\ndf_credit_risk = remove_outliers(df_credit_risk)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### setting and spliting the data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Convert to appropriate data types\nfor column in ['duration', 'credit_amount', 'installment_commitment', 'residence_since', 'age', 'existing_credits', 'num_dependents']:\n    df_credit_risk[column] = pd.to_numeric(df_credit_risk[column], errors='coerce')\n\n# Label encoding for target variable 'class'\nle = LabelEncoder()\ndf_credit_risk['class'] = le.fit_transform(df_credit_risk['class'])\n\n# Convert categorical variables into dummy/indicator variables\ncategorical_columns = ['checking_status', 'credit_history', 'purpose', 'savings_status', 'employment', 'personal_status', \n                       'other_parties', 'property_magnitude', 'other_payment_plans', 'housing', 'job', 'own_telephone', \n                       'foreign_worker']\ndf_credit_risk = pd.get_dummies(df_credit_risk, columns=categorical_columns)\n\n\n\n# Define features and target\nX = df_credit_risk.drop('class', axis=1)\ny = df_credit_risk['class'].values\n\n# Imputation and scaling\nimputer = SimpleImputer()\nscaler = StandardScaler()\nX_imputed = imputer.fit_transform(X)\nX_scaled = scaler.fit_transform(X_imputed)\n\n# Feature importance\nmodel = GradientBoostingClassifier()\nmodel.fit(X_scaled, y)\nfeature_importance = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n\n# Define top 5 features\nfeatures_top_5 = ['checking_status_no checking', 'credit_amount', 'duration', 'age', 'checking_status_<0']\n\n# Subset features\nX_subset = X[features_top_5]\n\n# Imputation and scaling for subset features\nX_subset_imputed = imputer.fit_transform(X_subset)\nX_subset_scaled = scaler.fit_transform(X_subset_imputed)\n# Define model\nmodel = SVC()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "In the code above, data type Conversion takes place in columns converting them to numeric data types and replacing non-numeric values with NaNs.\nLabel Encoding also takes place; the target column chosen was 'class', and it is encoded from string labels to numeric values to be compatible with machine learning algorithms.\n\nAnother technique taking place is One-Hot Encoding; in the code above, the categorical columns are transformed into binary columns, converting categorical values into numerical data so the machine learning model can read.\n\nDefining Features and Target in the code above, the top five features with the most significant relationship to class became x and y, the variable targeted \"class\".\n\nImputation and Scaling: In this part of the code, any features with None values are scaled for mean 0 and standard deviation 1, making them ready for machine learning algorithms while not affecting the predictions as they become the averages.\n\nFeature Importance: Using Gradient Boosting, the importance of each feature in predicting the target is determined.\n\nSubsetting Top Features: The dataset is a subset of the top 5 essential features.\n\nPreparing Subset Data: The subset data also undergoes imputation and scaling, similar to the entire dataset.\n\nModel Definition: Finally, a Support Vector Classifier model is defined, ready for training on the prepared data.\n\nOverall, the script transforms and pre-processes the credit risk data, assesses feature significance, and sets the stage for using an SVM-based predictive model.\n\n\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Run the model 10 times with different random seeds\naccuracy_scores = []\nf1_scores = []\nconfusion_matrices = []\n\nfor i in range(10):\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X_subset_scaled, y, test_size=0.35, random_state=i)\n\n    # Train model\n    model.fit(X_train, y_train)\n\n    # Make predictions\n    y_pred = model.predict(X_test)\n\n    # Get accuracy score, F1 score, and confusion matrix\n    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n\n    # Store scores and confusion matrix\n    accuracy_scores.append(accuracy)\n    f1_scores.append(f1)\n    confusion_matrices.append(cm)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Initialization: In the code above, three empty lists are created, and they are: accuracy_scores, f1_scores, and confusion_matrices to store evaluation metrics across the ten seeds.\n\nLoop to Train and Test Model: A for loop is initialized, which runs ten times, each time with a different random seed i.\n\nData Splitting: The dataset is split inside the loop into training and testing sets using the train_test_split function.\nThe test_size  is 35% of the dataset, and it will be used for testing, while the remaining 65% will be used for training which is a standard split.\n\nThe random_state argument is set to the current loop iteration number i. This ensures a different random seed for each iteration, which will show the robustness of the model.\n\nModel Training: The SVM model was chosen and is trained using the fit method on the training data (X_train and y_train).\n\nModel Prediction: After complete training, the model then predictions (y_pred) are made on the test dataset (X_test).\nEvaluation: When creating the code, three evaluation metrics are computed for each iteration:\nFirstly the accuracy of the model is taken; it measures the fraction of correct predictions out of all predictions.\n\nSecondly, the f1 is added to the list; the f1 score is instrumental as it is used to show class distribution.\n\nLastly, cm was taken, the confusion matrix, which provides a detailed breakdown of true positive, true negative, false positive, and false pessimistic predictions. These metrics are then appended to their respective lists for graphing of the results later.\n\nResult: After successfully running the code ten times, the three lists have ten values in them to graph the values.\n\nBy running the SVM model multiple times with different train-test splits, this approach provides a more robust estimate of the model's performance. It accounts for potential variability due to different data splits, reducing the likelihood that a particularly \"good\" or \"bad\" split biases the performance assessment.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Display the Accuarcy and the F1 score \nprint(f\"Model: Support Vector Machine\")\nprint(f\"Mean Accuracy: {np.mean(accuracy_scores)}\")\nprint(f\"Mean F1 Score: {np.mean(f1_scores)}\")\nprint(f\"Confusion Matrices: \")\nfor matrix in confusion_matrices:\n    print(matrix)\n\n# Plot distribution of the varibales \nfig, ax = plt.subplots(1, 2, figsize=(15, 7))\n\n# Dot plot for Accuracy Scores\nsns.stripplot(y=accuracy_scores, ax=ax[0], color='b', jitter=0.4)\nax[0].set_title('Distribution of Accuracy Scores')\nax[0].set_ylabel('Accuracy Score')\n\n# Dot plot for F1 Scores\nsns.stripplot(y=f1_scores, ax=ax[1], color='g', jitter=0.4)\nax[1].set_title('Distribution of F1 Scores')\nax[1].set_ylabel('F1 Score')\n# display them together\nplt.tight_layout()\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "As we can see the model has a range of an 71% - 77% accuracy which means that the model is not that accurate when it comes to predicting, but when we examine the F1 we can see it has a range from 80% - 88%. The F1 score should be used as it provides a better insight into the class distribution.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Plot heatmap of confusion matrix\ncm_avg = np.mean(confusion_matrices, axis=0)  # Average confusion matrix\nsns.heatmap(cm_avg, annot=True, cmap='Blues')\nplt.title('Confusion Matrix Heatmap')\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "When the heatmap is being examined, the features that came up are:\n\nTrue Positives = 28: The model correctly predicted 28 instances as the positive class.\n\nFalse Positives = 69: The model incorrectly predicted 69 instances as the positive class when they were the negative class. For this part, it means that the model made 69 incorrect predictions.\n\nFalse Negatives = 17: This part shows that the model has made another 17 incorrect errors, meaning there were 17 type II errors.\n\nTrue Negatives = 220: The model correctly predicted 220 instances as the negative class.\n\nLooking at the data above, it becomes clear that the model is good at predicting negative cases but needs to catch up when it comes to predicting positive cases. The data did not have a good split showing good and bad credit. To increase the accuracy of the model, more data showing the difference in good and bad credit would need to be provided for the model to shine.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Summary\n\nThe credit risk prediction model displayed commendable performance, achieving an accuracy that ranged between 71% and 77%. Notably, while accuracy provides a general sense of model correctness, the F1 score, ranging between 80% and 88%, offers a more balanced perspective on the model's capability in handling both positive and negative classes.\nUpon inspecting the confusion matrix, specific insights emerge:\nTrue Positives = 28: The model identified 28 cases as the positive class.\nFalse Positives = 69: A matter of concern arises as the model falsely classified 69 instances as positive when they belonged to the negative class.\nFalse Negatives = 17: Further, the model made an additional 17 erroneous predictions, which resulted in type II errors.\nTrue Negatives = 220: On the brighter side, the model successfully recognises 220 instances as the negative class.\nWhen exploring the models' performance, it becomes apparent that it can discern negative instances but falters slightly regarding the positive ones. When diving into the data set provided, it can be theorised that the dataset may be skewed in favour of good credit, meaning we need more examples of bad credit. Such an imbalance can lead the model to develop a bias.\nTo enhance the precision of the model, there must be changes to the data and the model, and they are:\nData Augmentation: Improving instances of bad credit will grant the model a broader perspective, enabling it to differentiate between good and bad credit more proficiently. When direct data collection becomes challenging, other methods need to be deployed.\nModel Exploration: Venturing beyond the current algorithm to explore models that naturally handle class imbalances, such as Random Forest or XGBoost, could lead to improvements.\nFeature Enhancement: Introducing or refining new features based on domain expertise can further sharpen the model's discerning power. Domain-specific insights into credit risk factors can pave the way for innovative feature engineering.\nMetric Optimisation: There are better ideas than relying on accuracy when making a good model. It is essential to shift the focus to other metrics like precision, recall, or even the F1 score can offer a more detailed and balanced evaluation. Optimising the recall value can be instrumental, as when dealing with credit, it is crucial to develop a model that gives the most accurate predictions so the banks can give loans to the correct people.\nTo conclude, the model, while proficient, definitely needs improvement. Credit risk evaluation is critical as many individuals apply for loans but need more funds to repay the banks.",
      "metadata": {}
    }
  ]
}