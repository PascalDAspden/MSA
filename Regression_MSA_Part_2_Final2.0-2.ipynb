{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Regression MSA Part 2\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Explaining the variables\nWork_year: This feature refers to the number of years an individual has been in the workforce. It is a numeric variable.\n\nExperience_level: This categorical variable indicates the individual's experience level. This gives an idea about the expertise and seniority of the individual in their field.\n\nEmployment_type: This is also a categorical variable which describes the type of employment the individual holds.\n\nJob_title: This is another categorical feature which gives us insight into the individual's job title.\n\nSalary: This is a continuous numeric variable that indicates the salary earned by the individual. \n\nSalary_currency: This is another categorical variable that represents the currency the salary of employees is paid in. The currency range from 'USD', to'GBP'.\n\nSalary_in_usd: This numeric variable represents the individual's salary in USD. It is calculated based on the original salary and the exchange rate from the salary currency to USD.\n\nEmployee_residence: This categorical variable indicates the country where the employee lives and works from.\n\nRemote_ratio: This is a numerical variable being either 100 or 0.\n\nCompany_location: This is a categorical variable indicating the location of the company where the individual is employed, such as the US.\n\nCompany_size: This is a categorical variable which indicates the company's size. \n\nThese are all the features given to me to find my target, the 'salary_in_usd' feature, as this feature does not need any conversion (i.e., converting currencies).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Importing libaries and data_salaries",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n# --- Load data ---\ndf_salaries = pd.read_csv('data_salaries.csv')\n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## setting and spliting the data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# For numerical features\nnumerical_features = df_salaries.select_dtypes(include=[np.number])\nnumerical_summary = numerical_features.describe().transpose()\n\n# For categorical features\ncategorical_features = df_salaries.select_dtypes(include=['object'])\ncategorical_summary = categorical_features.describe(include=['object']).transpose()\n\ndf_encoded_residence = pd.get_dummies(df_salaries['employee_residence'])\ndf_encoded_job_title = pd.get_dummies(df_salaries['job_title'])\n\n# Drop the original 'employee_residence', 'job_title', and 'Profession' columns\ndf_salaries = df_salaries.drop(columns=['employee_residence', 'job_title'])\n\n# Join the encoded data frame with the original ones\ndf_salaries = df_salaries.join(df_encoded_residence)\ndf_salaries = df_salaries.join(df_encoded_job_title)\n\n# Convert ordinal variables to numerical\ndf_salaries['company_size'] = df_salaries['company_size'].map({'S': 0, 'M': 1,'L':2})\ndf_salaries['experience_level'] = df_salaries['experience_level'].map({'SE': 0, 'MI':1,'EN':2,'EX':3})\ndf_salaries['employment_type'] = df_salaries['employment_type'].map({'FT': 0, 'CT':1,'FL':2,'PT':3})\n# Function to remove outliers\ndef remove_outliers(df):\n    numeric_cols = df.select_dtypes(include=[np.number])\n    mean = numeric_cols.mean()\n    std = numeric_cols.std()\n    is_outlier = (np.abs(numeric_cols - mean) > 3 * std).any(axis=1)\n    return df[~is_outlier]\ndf_salaries = remove_outliers(df_salaries)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "In the code above I decided it was best to do one-Hot Encoding, by doing so I can convert categorical variables into a binary format and appends them to the main data frame, while removing the original columns.\nOrdinal Conversion by doing this I was able to convert them into numerical form so the model can use them.\nOutlier Removal was also performed to make the model more accurate. \n\nThe model chosen was RandomForestRegressor and it works by creating multiple decision trees from the training data provided by MSA, the trees work by using bootstrapping and random feature selection. Each tree in the forest makes a prediction for a given data, and the final prediction is the average of all trees' generated predictions. This method is excellent as it reduces overfitting while also being able to handle mixed feature types and missing data which can be a common problem in any data set, but maybe computationally intensive if using a lot of data.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Chosing which features not to target for the model\nX = df_salaries.drop(columns=['salary_in_usd','salary_currency','company_location','work_year'])\ny = df_salaries['salary_in_usd']\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train the model\nrf = RandomForestRegressor(n_estimators=100, random_state=777)\nrf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf.predict(X_test)\n\n# Print the Root Mean Squared Error\nprint(\"Root Mean Squared Error: \", np.sqrt(mean_squared_error(y_test, y_pred)))\nmin_salary = df_salaries['salary_in_usd'].min()\nmax_salary = df_salaries['salary_in_usd'].max()\n\nprint(\"Minimum salary: \", min_salary)\nprint(\"Maximum salary: \", max_salary)\n\n# Calculate the Mean Absolute Error\nmae = mean_absolute_error(y_test, y_pred)\nprint(\"Mean Absolute Error: \", mae)\n\n# Calculate R-squared\nr2 = r2_score(y_test, y_pred)\nprint(\"R-squared: \", r2)\n\n# define a range of seeds\nseeds = range(10)\n\nmae_values = []\nr2_values = []\n\n# loop over the seeds\nfor seed in seeds:\n\n    # Split the dataset into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)\n\n    # Train the model\n    rf = RandomForestRegressor(n_estimators=100, random_state=777)\n    rf.fit(X_train, y_train)\n\n\n    # Make predictions\n    y_pred = rf.predict(X_test)\n\n    # Calculate the Mean Absolute Error and R2 score\n    mae = mean_absolute_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    # append the scores to their respective lists\n    mae_values.append(mae)\n    r2_values.append(r2*1000)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Data Splitting: I began by dividing the data into training and test sets for my salary prediction project.\n\nPrediction: After training the model on the training data, I made predictions on the test data to assess how well the model generalizes to unseen data.\n\n\nSeed Variation: To ensure the true accuracy I conducted a seed variation analysis meaning it runs through random seeds. By changing the random seed for data splitting, I observed and plotted the changes in the model's performance metrics. This analysis showed that the model's performance remained consistent across different train-test splits, indicating its robustness and accuracy.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# plot the results\nplt.plot(seeds, mae_values, label='Mean Absolute Error')\nplt.plot(seeds, r2_values, label='R-squared')\nplt.xlabel('Seed')\nplt.ylabel('Value')\n\nfor i, txt in enumerate(mae_values):\n    plt.text(seeds[i], mae_values[i], f'{txt:.2f}', ha='center')\n\nfor i, txt in enumerate(r2_values):\n    plt.text(seeds[i], r2_values[i], f'{txt:.2f}', ha='center')\n\nplt.legend()\nplt.title('The difference between the Root Mean Error and the R-squared over 10 seeds')\nplt.show()\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Understanding MAE: Mean Absolute Error is a commonly used metric in regression analysis. The model calculates MAE. The MAE value provides penalty by comparing the actual value to the predicted one. It is also a good indication of the model's predictions. The MAE It had a range from 1059-1694, which is very low as the range of earnings is 5132-324000.\n\nOverall, the combination of model training, cleaning, and seed variation helps to has provided valuable insights into the performance of this salary prediction model, helping me make informed decisions about its suitability for real-world applications.\n\nEvaluation: To measure the model's performance, I calculated several key metrics, including the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R2). By running the code with 10 random seeds I was able to produce a pattern showing the difference over the seeds, proving that the model is robust and fits multiple seeds. I also times the R2 values by 1000 to see the difference more clearly, as it was to small to visualise the numbers without it.\n\n- The MAE value of 1546 indicates that, on average, my model's predictions were about $1546 away from the actual salary values.\n\n- The R-squared (R2) score of 0.98 suggests that approximately 98% of the variation in salary could be explained by the features in the model. R2 is a measure of how well the model fits the data as predicted a higher values leads to a better fit.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Summary \n\nIn this part of MSA, the objective was to predict salary in USD based on various features provided in the data set, using the model of our choice. First, the data was preprocessed by turning the data into numerical form. It was also required to perform one-hot encoding for categorical variables into numeric values so the model could read it. The preprocessed data was then split into a training set and a test set the split being 30/70.\n\nA RandomForestRegressor model was trained on the training data using 100 estimators. The model was then used to make predictions based on the data provided, and its performance was later then evaluated using metrics that are used to test regression data, such as the Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R2). The MAE value of 1323 indicates that the model's predictions were, on average, about $1323 away from the actual values. The R2 score of 0.98 implies that 98% of predictions were correct, demonstrating outstanding model performance.\n\nFurthermore, a seed variation analysis was conducted to test the stability of the model's performance. The random seed for data splitting was varied, and the changes in the model's performance metrics were observed, and there was a slight variation which is to be expected. This analysis showed the model's robustness to different training and test set configurations.\n\nAdditionally, feature importances from the RandomForestRegressor model were analysed to gain deeper insights into the factors influencing salary predictions. This analysis offered valuable information from job seekers to employees wanting to know how much people should earn. Moreover, exploring other machine learning algorithms or fine-tuning model parameters might present opportunities for further enhancing the predictive performance of the salary prediction model; also, by increasing the amount of data, the model would be more accurate, but it would take longer as the model will take longer to compute.\n\nExamining the feature in the data helped shed light on which features are most influential in predicting salary, potentially informing job seekers or recruiters about the most significant factors they should look for in employees and how much they are \"worth\". Further tuning the model parameters or trying other machine learning algorithms could improve the model's performance.\n",
      "metadata": {}
    }
  ]
}